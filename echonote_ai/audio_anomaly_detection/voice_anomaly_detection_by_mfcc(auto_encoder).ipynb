{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_1ifJZ8Bqs9"
   },
   "source": [
    "# 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_ce5jM9Br3U"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "os.environ['CUDA_HOME']='/home/j-j11a210/.conda/envs/voice_strength'\n",
    "os.environ['LD_LIBRARY_PATH']='/home/j-j11a210/.conda/envs/voice_strength/lib'\n",
    "\n",
    "\n",
    "# 음성 데이터 처리\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 시각화\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 모델 관련\n",
    "import sklearn\n",
    "from sklearn import preprocessing # AttributeError: module 'sklearn' has no attribute 'preprocessing'\n",
    "import tensorflow as tf\n",
    "# import keras\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "# from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtbBkaIo10Ix"
   },
   "source": [
    "# 2. GPU 인식되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llPV_VfB114Q",
    "outputId": "3a6cc1fd-5589-469d-bb41-e51a09eb9a71"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALS14c29BaXm"
   },
   "source": [
    "# 3. 이것저것 필요한 변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uFPZ3e3BolF"
   },
   "outputs": [],
   "source": [
    "# sample rate at loaded\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# mfcc parameter\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "N_MELS = 64\n",
    "\n",
    "# slicing window parameter\n",
    "# 입력에 대해 일정 단위로 잘라서 모델에 넣는다.\n",
    "WINDOW_SECOND = 30\n",
    "HOP_SECOND = 0\n",
    "\n",
    "# result data\n",
    "mfcc_list = []\n",
    "feature_list = []\n",
    "\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cglxbCJAZis1"
   },
   "source": [
    "# 4. 데이터 가공 관련 메소드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ja8McVkd5AF0"
   },
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    # 오디오 파일 로드\n",
    "    assert os.path.isfile(file_path), \"Wrong path to audio file\"\n",
    "    amplitude, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "\n",
    "    # WINDOW_SECOND가 10s이므로 파일이 10초보다 짧거나 HOP_SECOND의 배수가 아닌 경우 패딩을 추가합니다.\n",
    "    window_frame = WINDOW_SECOND * SAMPLE_RATE\n",
    "\n",
    "    if len(amplitude) % window_frame != 0:\n",
    "      added_frame = window_frame - (len(amplitude) % window_frame)\n",
    "      amplitude = librosa.util.fix_length(data=amplitude, size=len(amplitude) + added_frame)\n",
    "\n",
    "\n",
    "    # input audio의 길이가 WINDOW_SECOND 보다 짧은 경우 (480000,)의 1차원 배열\n",
    "    # print(\"amplitude shape: \" + str(amplitude.shape))\n",
    "\n",
    "    return amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDW0T_emsBWw"
   },
   "outputs": [],
   "source": [
    "def extract_mfcc(frame):\n",
    "    print(\"frame shape: \" + str(frame.shape))\n",
    "    mfcc = librosa.feature.mfcc(y=frame, sr=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "\n",
    "    # (n_mfcc, time_steps)를 전치시켜 시간에 따른 특성을 학습하도록 한다.\n",
    "    mfcc = mfcc.T\n",
    "    print(\"mfcc shape: \" + str(mfcc.shape)) # (3001, 20)\n",
    "\n",
    "    # Z-score로 정규화 후 리턴 (keras의 예제에서도 이 방식을 선택)\n",
    "    return sklearn.preprocessing.minmax_scale(mfcc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdHaEyzx410e"
   },
   "outputs": [],
   "source": [
    "# 슬라이딩 윈도우를 적용해 오디오를 일정 크기의 프레임 단위로 나누는 함수\n",
    "def audio_sliding_window(file_path, window_second=WINDOW_SECOND, hop_second=HOP_SECOND):\n",
    "    amplitude = load_audio(file_path)\n",
    "\n",
    "    # window_second(초)와 hop_second(초)를 샘플 단위로 변환\n",
    "    window_samples = window_second * SAMPLE_RATE\n",
    "    hop_samples = window_samples # 이전: hop_second * SAMPLE_RATE\n",
    "\n",
    "    # 프레임 단위로 오디오를 슬라이딩 윈도우 기법으로 분할\n",
    "    frame_list = librosa.util.frame(amplitude, frame_length=window_samples, hop_length=window_samples)\n",
    "    # print(\"sliced frame shape: \" + str(frame_list.shape))\n",
    "\n",
    "    return frame_list.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4BaPv8oC-O1"
   },
   "outputs": [],
   "source": [
    "def create_train_set(directory, audio_file_list):\n",
    "    feature_list = []\n",
    "    \n",
    "    # 디렉토리 내 모든 오디오 파일에 대해\n",
    "    audio_file_count = len(audio_file_list)\n",
    "    for index, audio_file in enumerate(audio_file_list):\n",
    "      print(str(index) + \"/\" + str(audio_file_count) + \": \" + audio_file + \" pre-prossessing...\")\n",
    "\n",
    "\n",
    "      # 오디오를 WINDOW_SECOND초 단위로 자른다.\n",
    "      audio_full_path = os.path.join(directory, audio_file)\n",
    "      frame_list = audio_sliding_window(audio_full_path, WINDOW_SECOND, HOP_SECOND)\n",
    "\n",
    "      for frame in frame_list:\n",
    "        new_frame = extract_mfcc(frame)\n",
    "        feature_list.append(new_frame)\n",
    "        print(\"new_frame shape: \" + str(new_frame.shape))\n",
    "\n",
    "    return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huJ1tA8o3vPV"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddxIk-rFSQSF"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8_kJAXoY8Mf"
   },
   "source": [
    "## 1) 학습셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6lFlqlSDBlV",
    "outputId": "96e4e95b-c643-4075-fb27-0233cbc02899"
   },
   "outputs": [],
   "source": [
    "# 디렉토리에서 wav 파일 목록을 불러온다.\n",
    "directory = r'/home/j-j11a210/EchoNote/AI/train_data'\n",
    "\n",
    "audio_full_path_list = []\n",
    "\n",
    "for (path, dir, files) in os.walk(directory):\n",
    "    print(path)\n",
    "    for filename in files:\n",
    "        if filename.endswith('.wav'):\n",
    "            audio_full_path_list.append(os.path.join(path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5p__HCFc32H9",
    "outputId": "f3a41a34-8919-437b-b1e2-dde08c945458"
   },
   "outputs": [],
   "source": [
    "# 학습셋을 생성한다.\n",
    "train_set = create_train_set(directory, audio_full_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNMXxACQeQgO",
    "outputId": "ef8f8d42-fcde-445d-c279-d78e59208775"
   },
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "librosa.display.specshow(train_set[0].T, sr=SAMPLE_RATE, x_axis='time') # 그릴 때는 전치\n",
    "plt.title(\"Original Data\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji6YmvR2Y7NW"
   },
   "source": [
    "## 2) 모델 구축 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfT0gjJvS3k5",
    "outputId": "330b1618-47ce-4b7f-890d-18f52757bb35"
   },
   "outputs": [],
   "source": [
    "def create_autoencoder(input_shape):\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "    encoded = tf.keras.layers.Dense(128, activation='relu')(input_layer)\n",
    "    encoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\n",
    "\n",
    "    decoded = tf.keras.layers.Dense(128, activation='relu')(encoded)\n",
    "    decoded = tf.keras.layers.Dense(input_shape[1], activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = tf.keras.models.Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 및 학습\n",
    "model = create_autoencoder(input_shape=(train_set.shape[1], train_set.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(\n",
    "    train_set,\n",
    "    train_set,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gccv5NyddbTJ"
   },
   "source": [
    "## 3) 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "DQ14QtlTdczh",
    "outputId": "821d74fd-fa13-45d3-91b2-5f4b455c6507"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(train_history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGSaumMMdix5"
   },
   "source": [
    "## 4) 재구성 오류 계산을 통한 이상 현상 감지\n",
    "오토인코더는 입력 데이터를 압축한 후 다시 복원하는 과정을 거친다. **입력 데이터(원본 데이터)**를 인코더로 압축하여 저차원 잠재 공간(z)으로 변환, 그 후, 디코더를 통해 잠재 공간(z)에서 복원된 데이터를 생성한다.\n",
    "\n",
    "재구성 오류는 입력 데이터와 복원된 데이터 간의 차이를 나타낸다.\n",
    "재구성 오류(Reconstruction Error)란, 원본 데이터와 오토인코더가 복원한 데이터 간의 차이를 나타내는 값으로, 오토인코더가 원본 데이터를 얼마나 잘 복원했는지를 나타낸다.\n",
    "\n",
    "정상적인 데이터는 오토인코더가 잘 복원할 수 있어 재구성 오류가 작다. 오토인코더가 학습에 사용한 정상적인 패턴에 가까운 데이터를 입력하면 복원 과정에서 큰 손실 없이 재구성할 수 있다.\n",
    "\n",
    "반면, 이상치 데이터는 복원이 잘 되지 않아 재구성 오류가 크다. 이상치는 학습 중에 경험하지 못한 패턴일 가능성이 높으므로, 오토인코더가 이를 잘 복원하지 못해 큰 오류가 발생한다.\n",
    "\n",
    "---\n",
    "\n",
    "### 그래프에 대한 설명\n",
    "각 loss에 대한 데이터 수를 의미하는 것 같다. loss가 0.00에 가까운 sample이 대다수임을 볼 수 있다. 또한 이상 경계값인 0.04를 넘는 샘플이 거의 없다. 이런 이유로 해당 데이터에서 anomaly가 발견되지 않았다고 할 수 있다?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "EM7GFZYUduMR",
    "outputId": "465ae2b5-4e39-4e6c-f671-8d078b1c2d6a"
   },
   "outputs": [],
   "source": [
    "# mae loss를 계산해보자.\n",
    "\n",
    "# 모델을 이용해 입력 데이터를 복원한다.\n",
    "train_pred = model.predict(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 복원한 결과와 실제 원본값의 차이를 계산한다.\n",
    "# 차이가 클수록 오토인코더가 복원하는 데 어려움을 겪었다는 의미\n",
    "# == 해당 데이터가 이상치일 가능성이 있다는 의미\n",
    "# ↓↓↓↓↓↓ 생각보다 오래걸림\n",
    "train_mae_loss = np.mean(np.abs(train_pred - train_set), axis=-1)\n",
    "\n",
    "# ?\n",
    "plt.hist(train_mae_loss[0], bins=50)\n",
    "# plt.xlabel(\"Train mae loss\")\n",
    "# plt.ylabel(\"mae samples\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0Gqs3I-ndvE"
   },
   "source": [
    "## 5) 재구성이 잘 되었는지 시각화하여 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "7b_jCbDCkSLp",
    "outputId": "9c189847-1d25-44b7-871e-0e38eba001e5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "mfcc라서 그냥 matplotlib으로 바로 그리면 안 되고\n",
    "librosa.display.specshow 메서드를 써서 그려야 한다.\n",
    "keras의 예제는 mfcc를 쓰지 않았기에 그냥 matplotlib으로 그린 것.\n",
    "'''\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "librosa.display.specshow(train_set[100].T, sr=SAMPLE_RATE, x_axis='time', hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "plt.title('Original Data: {}'.format(audio_full_path_list[0]))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "librosa.display.specshow(train_pred[100].T, sr=SAMPLE_RATE, x_axis='time', hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "plt.title(\"Reconstitution Data\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kqc2Y5kDnm4s"
   },
   "source": [
    "## 6) 이상 탐지 임계값 설정\n",
    "평균 재구성 오류에 3배의 표준편차를 더한 값을 임계값으로 사용한다?\n",
    "\n",
    "통계적으로 정규 분포를 가정하면, 평균 ± 3표준편차 안에 약 99.7%의 데이터가 포함된다? 즉, 정상적인 범위는 평균 재구성 오류에서 3표준편차 내의 값이라고 볼 수 있으며, 그보다 큰 값들은 이상치로 간주할 수 있다?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_mae_loss[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sklearn.preprocessing.minmax_scale(train_mae_loss[5], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "editable": true,
    "id": "_ARc_wTViLQV",
    "outputId": "def6425b-5bc3-4594-f324-dd26349773c4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 일반적인 방식\n",
    "# # Get reconstruction loss threshold.\n",
    "# threshold = np.mean(train_mse_loss) + 3 * np.std(train_mse_loss)\n",
    "# print(\"Reconstruction error threshold: \", threshold)\n",
    "\n",
    "threshold_mean = np.mean(train_mae_loss, axis=-1) + 3 * np.std(train_mae_loss)\n",
    "threshold_max = np.max(train_mae_loss)\n",
    "print(\"Reconstruction error threshold_mean: \", threshold_mean)\n",
    "print(\"Reconstruction error threshold_max: \", threshold_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 각 차원에 대한 임계값 설정\n",
    "# 임계값은 세로 방향으로 계산해야 한다.\n",
    "thresholds = np.mean(train_mae_loss, axis=0) + 3 * np.std(train_mae_loss, axis=0)\n",
    "print(\"Reconstruction error thresholds: \", thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 time frame 마다 임계값이 생성되었음을 확인\n",
    "thresholds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TBNIwYgpAdB"
   },
   "source": [
    "## 7) 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdgqL7amo_7U",
    "outputId": "b165ad31-bbdb-49f0-a820-91d0b27af0e2"
   },
   "outputs": [],
   "source": [
    "# @keras.saving.register_keras_serializable()\n",
    "model.save(r'/home/j-j11a210/EchoNote/AI/audio_anomaly_detection_mae.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XfzY5PBYher"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실 데이터 불러오기\n",
    "test_data = create_train_set(r'/home/j-j11a210/EchoNote/AI', ['hanseokwon44k.wav'])\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_list = model.predict(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "librosa.display.specshow(test_data[0].T, sr=SAMPLE_RATE, x_axis='time', hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "plt.title('Original Data')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "librosa.display.specshow(test_predict_list[0].T, sr=SAMPLE_RATE, x_axis='time', hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "plt.title(\"Reconstitution Data\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_loss_list = np.mean(np.abs(test_data - test_predict_list), axis=-1) # (n, 3001)\n",
    "print(mae_loss_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"MAE loss\")\n",
    "plt.plot(sklearn.preprocessing.minmax_scale(mae_loss_list[0], axis=0))\n",
    "# plt.axhline(y=threshold_max, color='r', linewidth=1)\n",
    "# plt.axhline(y=threshold_mean, color='g', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(test_data)\n",
    "test_mae_loss = np.mean(np.abs(test_predict - test_data), axis=-1)\n",
    "test_threshold = np.mean(test_mae_loss) + 3 * np.std(test_mae_loss)\n",
    "\n",
    "# librosa.display.specshow(, sr=SAMPLE_RATE, x_axis='time')\n",
    "for time_frame in test_mae_loss:\n",
    "    plt.plot(time_frame)\n",
    "    plt.axhline(y=test_threshold, color='r', linestyle='--')\n",
    "    plt.title('Reconstruction Error for New Audio')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIl1o_YYEuwH",
    "outputId": "901bb4ca-e11d-4374-af48-073bf8221b82"
   },
   "outputs": [],
   "source": [
    "# 7. 이상치 탐지\n",
    "print(test_mae_loss.shape)\n",
    "print(\"loss: \", test_mae_loss)\n",
    "anomalies_new = np.where(test_mae_loss > threshold)[0]\n",
    "print(\"Detected anomalies at:\", anomalies_new)\n",
    "\n",
    "# 8. 이상치가 발생한 시점 출력\n",
    "# 예를 들어 이상치가 발생한 프레임이 몇 초인지 계산\n",
    "frame_duration = len(test_data) / len(test_mae_loss)  # 한 프레임이 차지하는 시간 비율 계산\n",
    "anomalous_times = anomalies_new * frame_duration\n",
    "print(\"Anomalies detected at (seconds):\", anomalous_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DUKg2broEPgk",
    "outputId": "98afedb5-fdc4-47b4-b560-c26d5584cb68"
   },
   "outputs": [],
   "source": [
    "# test_predict = model.predict(test_data)\n",
    "# test_mae_loss = np.mean(np.power(test_predict - test_data, 2), axis=1)\n",
    "# threshold = np.mean(test_mae_loss) + 3 * np.std(test_mae_loss)\n",
    "\n",
    "# # librosa.display.specshow(, sr=SAMPLE_RATE, x_axis='time')\n",
    "# for time_frame in test_mae_loss:\n",
    "#     plt.plot(time_frame)\n",
    "#     plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "#     plt.title('Reconstruction Error for New Audio')\n",
    "#     plt.show()\n",
    "\n",
    "# 타임스탬프 생성 (예: 1초 간격의 타임스탬프)\n",
    "timestamps = pd.date_range(start='2024-10-01', periods=len(test_mae_loss), freq='S')\n",
    "\n",
    "# 각 차원에 대한 이상치 탐지\n",
    "anomalies = np.any(test_mae_loss > thresholds, axis=1)\n",
    "\n",
    "# 이상치가 발생한 시각 추출\n",
    "anomaly_timestamps = timestamps[anomalies]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Detected anomalies in test data:\", anomalies)\n",
    "print(\"Number of anomalies detected:\", np.sum(anomalies))\n",
    "print(\"Timestamps of detected anomalies:\", anomaly_timestamps)\n",
    "\n",
    "# 이상치 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(20):  # 13개의 MFCC 계수에 대한 시각화\n",
    "    plt.subplot(4, 5, i + 1)  # 3x5 서브플롯\n",
    "    plt.hist(test_mae_loss[:, i], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.axvline(x=thresholds[i], color='red', linestyle='--', label='Threshold')\n",
    "    plt.title(f'MFCC {i + 1}')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlppueDdVMjz"
   },
   "outputs": [],
   "source": [
    "def sec_to_timestamp(sec):\n",
    "    hour = sec // (60*60)\n",
    "    sec %= 60*60\n",
    "\n",
    "    minute = sec // 60\n",
    "    sec %= 60\n",
    "\n",
    "    return '%02d:%02d:%02d' % (hour, minute, sec)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "voice_strength",
   "language": "python",
   "name": "voice_strength"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
