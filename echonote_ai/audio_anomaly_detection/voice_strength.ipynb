{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 음성 데이터 처리\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 시각화\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 모델 관련\n",
    "import sklearn\n",
    "from sklearn import preprocessing # AttributeError: module 'sklearn' has no attribute 'preprocessing'\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# 노이즈 제거\n",
    "# from df import enhance, init_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GPU 관련 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "os.environ['CUDA_HOME']='/home/j-j11a210/.conda/envs/voice_strength'\n",
    "os.environ['LD_LIBRARY_PATH']='/home/j-j11a210/.conda/envs/voice_strength/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepfilternet 사용 준비\n",
    "# dfn_model, df_state, _ = init_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/j-j11a210/EchoNote/AI/Voice_Strength/audio_anomaly_detection_mae.keras'\n",
    "\n",
    "# sample rate at loaded\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# mfcc parameter\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "N_MELS = 64\n",
    "\n",
    "# slicing window parameter\n",
    "# 입력에 대해 일정 단위로 잘라서 모델에 넣는다.\n",
    "WINDOW_SECOND = 30\n",
    "HOP_SECOND = 0\n",
    "\n",
    "# result data\n",
    "mfcc_list = []\n",
    "feature_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae 임계값\n",
    "threshold_mean = 0.02182374382391572\n",
    "threshold_max = 0.08 # 0.11855171\n",
    "\n",
    "# thresholds = np.array([5.9390197e-07, 1.4276836e-03, 1.2026135e-03, 1.0732074e-03, 7.0909178e-04,\n",
    "#  5.3973543e-04, 4.0763570e-04, 3.1770475e-04, 3.7403271e-04, 2.7342548e-04,\n",
    "#  2.8244586e-04, 2.5252026e-04, 3.4210877e-04, 2.9945065e-04, 2.2181304e-04,\n",
    "#  2.3136209e-04, 2.4996133e-04, 2.1270799e-04, 2.6198488e-04, 2.9478868e-04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 오디오 분석 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    # 오디오 파일 로드\n",
    "    assert os.path.isfile(file_path), \"Wrong path to audio file\"\n",
    "    amplitude, _ = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "    \n",
    "    # amplitude = librosa.to_mono(amplitude)\n",
    "    # amplitude = enhance(dfn_model, df_state, segment)\n",
    "\n",
    "\n",
    "    # WINDOW_SECOND가 10s이므로 파일이 10초보다 짧거나 HOP_SECOND의 배수가 아닌 경우 패딩을 추가합니다.\n",
    "    window_frame = WINDOW_SECOND * SAMPLE_RATE\n",
    "\n",
    "    if len(amplitude) % window_frame != 0:\n",
    "      added_frame = window_frame - (len(amplitude) % window_frame)\n",
    "      amplitude = librosa.util.fix_length(data=amplitude, size=len(amplitude) + added_frame)\n",
    "\n",
    "\n",
    "    # input audio의 길이가 WINDOW_SECOND 보다 짧은 경우 (480000,)의 1차원 배열\n",
    "    # print(\"amplitude shape: \" + str(amplitude.shape))\n",
    "\n",
    "    return amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(frame):\n",
    "    # print(\"frame shape: \" + str(frame.shape)) # (480000,)\n",
    "    mfcc = librosa.feature.mfcc(y=frame, sr=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "\n",
    "    # (n_mfcc, time_steps)를 전치시켜 시간에 따른 특성을 학습하도록 한다.\n",
    "    mfcc = mfcc.T\n",
    "    # print(\"mfcc shape: \" + str(mfcc.shape)) # (3001, 20)\n",
    "\n",
    "    # Z-score로 정규화 후 리턴 (keras의 예제에서도 이 방식을 선택)\n",
    "    return sklearn.preprocessing.minmax_scale(mfcc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 슬라이딩 윈도우를 적용해 오디오를 일정 크기의 프레임 단위로 나누는 함수\n",
    "def audio_sliding_window(file_path, window_second=WINDOW_SECOND, hop_second=HOP_SECOND):\n",
    "    amplitude = load_audio(file_path)\n",
    "\n",
    "    # window_second(초)와 hop_second(초)를 샘플 단위로 변환\n",
    "    window_samples = window_second * SAMPLE_RATE\n",
    "    hop_samples = window_samples # 이전: hop_second * SAMPLE_RATE\n",
    "\n",
    "    # 프레임 단위로 오디오를 슬라이딩 윈도우 기법으로 분할\n",
    "    frame_list = librosa.util.frame(amplitude, frame_length=window_samples, hop_length=window_samples)\n",
    "    # print(\"sliced frame shape: \" + str(frame_list.shape))\n",
    "\n",
    "    return frame_list.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_frame(directory, audio_file_list):\n",
    "    feature_list = []\n",
    "    \n",
    "    # 디렉토리 내 모든 오디오 파일에 대해\n",
    "    audio_file_count = len(audio_file_list)\n",
    "    for index, audio_file in enumerate(audio_file_list):\n",
    "      print(str(index) + \"/\" + str(audio_file_count) + \": \" + audio_file + \" pre-prossessing...\")\n",
    "\n",
    "\n",
    "      # 오디오를 WINDOW_SECOND초 단위로 자른다.\n",
    "      audio_full_path = os.path.join(directory, audio_file)\n",
    "      frame_list = audio_sliding_window(audio_full_path, WINDOW_SECOND, HOP_SECOND)\n",
    "\n",
    "      for frame in frame_list:\n",
    "        new_frame = extract_mfcc(frame)\n",
    "        feature_list.append(new_frame)\n",
    "        # print(\"new_frame shape: \" + str(new_frame.shape)) # (3001, 20)\n",
    "\n",
    "    return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_timestamp(sec):\n",
    "    hour = sec // (60*60)\n",
    "    sec %= 60*60\n",
    "\n",
    "    minute = sec // 60\n",
    "    sec %= 60\n",
    "\n",
    "    return '%02d:%02d:%02d' % (hour, minute, sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 사용 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "# 모델 로드\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abnormal_interval(directory, wav_file_list):\n",
    "    if str(type(wav_file_list)) != \"<class 'list'>\":\n",
    "        print(\"wav 파일 목록은 리스트로 주어져야 합니다.\")\n",
    "        return\n",
    "\n",
    "    try:    \n",
    "        # 실 데이터 불러오기\n",
    "        test_data = create_data_frame(directory, wav_file_list)\n",
    "        # test_data.shape # (n, 3001, 20)\n",
    "    \n",
    "        # 실제 predict(오토 인코더로 복원)\n",
    "        predict_data = model.predict(test_data, batch_size=32) # (n, 3001, 20)\n",
    "    \n",
    "        \n",
    "        return calc_abnormal_interval(test_data, predict_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"err: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_abnormal_interval(original_data, predict_data):\n",
    "    # mae loss 계산\n",
    "    mae_loss_list = np.mean(np.abs(original_data - predict_data), axis=-1) # (n, 3001)\n",
    "\n",
    "    test_threshold = np.mean(mae_loss_list) + 3 * np.std(mae_loss_list)\n",
    "    \n",
    "    # 이상값이 검출된 구간 탐색\n",
    "    anomalous_seconds = []\n",
    "    \n",
    "    for list_idx, mae_loss in enumerate(mae_loss_list):\n",
    "        # index = np.where(mae_loss > test_threshold)[0]\n",
    "        index = np.where((mae_loss > test_threshold) & (mae_loss > threshold_max))[0]\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    # print(index)\n",
    "    for i in index:\n",
    "        second = (i * HOP_LENGTH) / SAMPLE_RATE\n",
    "        anomalous_seconds.append(second + WINDOW_SECOND * list_idx)\n",
    "\n",
    "    print(anomalous_seconds)\n",
    "    return anomalous_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 데이터 시각화 (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 6))\n",
    "# librosa.display.specshow(test_data[0].T, sr=SAMPLE_RATE, x_axis='time', hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "# plt.title('Original Data')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(16, 6))\n",
    "# librosa.display.specshow(test_predict_list[0].T, sr=SAMPLE_RATE, x_axis='time', hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "# plt.title(\"Reconstitution Data\")\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"MAE loss\")\n",
    "# plt.plot(mae_loss_list[0])\n",
    "# plt.axhline(y=threshold_max, color='r', linewidth=1)\n",
    "# plt.axhline(y=threshold_mean, color='g', linewidth=1)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice_strength",
   "language": "python",
   "name": "voice_strength"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
