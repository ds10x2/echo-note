{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f507c8-5893-44e1-8a47-a807703150fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: openai-whisper in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (20231117)\n",
      "Requirement already satisfied: numba in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai-whisper) (2.4.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai-whisper) (4.66.5)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai-whisper) (10.5.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai-whisper) (0.7.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper) (74.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->openai-whisper) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install flask\n",
    "!pip install -U openai-whisper\n",
    "#!pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50995688-2279-4fb6-b85b-906938af9dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Worker 0 starting, loading Whisper model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://70.12.246.80:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "Exception in thread Thread-5 (worker):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_9324\\4036696997.py\", line 27, in worker\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\__init__.py\", line 156, in load_model\n",
      "    return model.to(device)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 7.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import os  # 운영 체제와 상호작용하기 위한 os 모듈 임포트\n",
    "import tempfile  # 임시 파일 생성을 위한 tempfile 모듈 임포트\n",
    "import logging  # 로그 메시지를 위한 logging 모듈 임포트\n",
    "from flask import Flask, request, jsonify  # 웹 애플리케이션 생성을 위한 Flask와 유틸리티 임포트\n",
    "import whisper  # Whisper ASR 모델 임포트\n",
    "import torch  # 텐서 연산을 위한 PyTorch 임포트\n",
    "import threading  # 스레드를 생성하기 위한 threading 모듈 임포트\n",
    "import queue  # 작업 큐 생성을 위한 queue 모듈 임포트\n",
    "import uuid  # 고유한 작업 ID 생성을 위한 uuid 모듈 임포트\n",
    "\n",
    "# 로그 메시지를 info 레벨로 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)  # 이 모듈을 위한 로거 생성\n",
    "\n",
    "app = Flask(__name__)  # Flask 애플리케이션 인스턴스 생성\n",
    "\n",
    "# 작업 큐와 결과 저장을 위한 딕셔너리 초기화\n",
    "task_queue = queue.Queue()\n",
    "results = {}\n",
    "\n",
    "# 오디오 파일을 처리할 작업자 스레드 수 설정\n",
    "num_worker_threads = 1\n",
    "\n",
    "# 오디오 파일을 처리할 작업자 함수 정의\n",
    "def worker(thread_id):\n",
    "    logger.info(f\"Worker {thread_id} starting, loading Whisper model...\")\n",
    "    whisper_model = whisper.load_model(\"medium\")  # Whisper ASR 모델 로드\n",
    "    logger.info(f\"Worker {thread_id} loaded Whisper model.\")\n",
    "\n",
    "    while True:  # 작업자가 계속 실행되도록 무한 루프\n",
    "        task_id, audio_path = task_queue.get()  # 큐에서 작업 가져오기\n",
    "        if task_id is None:  # 종료 신호 확인\n",
    "            break  # 종료 신호를 받으면 루프 종료\n",
    "        \n",
    "        logger.info(f\"Worker {thread_id} processing task {task_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Whisper 모델을 사용하여 오디오 파일을 전사\n",
    "            results[task_id] = {\"status\": \"processing\"}\n",
    "            result = whisper_model.transcribe(audio_path)\n",
    "            # segment에서 id, start, end, text \n",
    "            # 비동기 \n",
    "            # API통신.uri(result, \"spring url\")\n",
    "            \n",
    "            \n",
    "            results[task_id] = {\"status\": \"completed\", \"result\": result}  # 결과를 딕셔너리에 저장\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Worker {thread_id} encountered an error: {str(e)}\")  # 발생한 오류 로그\n",
    "            results[task_id] = {\"status\": \"failed\", \"error\": str(e)}  # 오류를 결과에 저장\n",
    "        finally:\n",
    "            os.remove(audio_path)  # 임시 오디오 파일 제거\n",
    "            if torch.cuda.is_available():  # CUDA 사용 가능 여부 확인\n",
    "                torch.cuda.empty_cache()  # 필요 시 CUDA 메모리 캐시 지우기\n",
    "        \n",
    "        task_queue.task_done()  # 작업 완료 표시\n",
    "\n",
    "# 작업자 스레드 시작\n",
    "threads = []\n",
    "for i in range(num_worker_threads):\n",
    "    t = threading.Thread(target=worker, args=(i,))  # 작업자 함수에 대한 새 스레드 생성\n",
    "    t.start()  # 스레드 시작\n",
    "    threads.append(t)  # 스레드 목록에 추가\n",
    "\n",
    "@app.route('/', methods=['GET'])  # 홈 페이지를 위한 라우트 정의\n",
    "def home():\n",
    "    return \"Welcome to the Asynchronous Flask Server with Whisper!\"  # 환영 메시지 반환\n",
    "\n",
    "@app.route('/stt', methods=['POST'])  # STT 요청을 위한 라우트 정의\n",
    "def stt_request():\n",
    "    logger.info(\"Received STT request\")\n",
    "    \n",
    "    if 'audio' not in request.files:  # 오디오 파일이 제공되었는지 확인\n",
    "        logger.warning(\"No audio file provided in the request\")\n",
    "        return jsonify({'error': 'No audio file provided'}), 400  # 제공되지 않으면 오류 반환\n",
    "    \n",
    "    audio_data = request.files['audio']  # 요청에서 오디오 파일 가져오기\n",
    "    \n",
    "    if audio_data.filename == '':  # 파일 이름이 비어 있는지 확인\n",
    "        logger.warning(\"Empty filename provided\")\n",
    "        return jsonify({'error': 'No selected file'}), 400  # 비어 있으면 오류 반환\n",
    "    \n",
    "    logger.info(f\"Processing audio file: {audio_data.filename}\")\n",
    "    \n",
    "    # 오디오 데이터를 저장할 임시 파일 생성\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp_audio:\n",
    "        audio_data.save(temp_audio.name)  # 오디오 데이터를 임시 파일에 저장\n",
    "        temp_audio_path = temp_audio.name  # 임시 파일 경로 저장\n",
    "    \n",
    "    # 고유한 작업 ID를 생성하고 작업을 큐에 추가\n",
    "    task_id = str(audio_data.filename)  # 작업을 위한 고유 식별자 생성\n",
    "    task_queue.put((task_id, temp_audio_path))  # 큐에 작업 추가\n",
    "    \n",
    "    logger.info(f\"Task {task_id} added to the queue\")\n",
    "    \n",
    "    return jsonify({\"task_id\": task_id, \"status\": \"processing\"}), 202  # 작업 ID와 상태 반환\n",
    "\n",
    "@app.route('/stt_result/<task_id>', methods=['GET'])  # 작업 상태를 확인하기 위한 라우트 정의\n",
    "def get_status(task_id):\n",
    "    if task_id not in results:  # 작업 ID가 결과에 없으면\n",
    "        return jsonify({\"status\": \"No_result\"}), 404  # 처리 중 상태 반환\n",
    "    \n",
    "    result = results[task_id]  # 주어진 작업 ID에 대한 결과 가져오기\n",
    "    \n",
    "    if result[\"status\"] == \"completed\":  # 작업이 완료되었는지 확인\n",
    "        del results[task_id]  # 메모리 해제를 위해 결과 딕셔너리에서 제거\n",
    "        return jsonify(result), 200  # 완료된 결과 반환\n",
    "    elif result[\"status\"] == \"failed\":  # 작업이 실패했는지 확인\n",
    "        del results[task_id]  # 메모리 해제를 위해 결과 딕셔너리에서 제거\n",
    "        return jsonify(result), 500  # 실패한 결과 반환\n",
    "    else:\n",
    "        return jsonify({\"status\": \"processing\"}), 202  # 여전히 처리 중인 상태 반환\n",
    "\n",
    "@app.errorhandler(500)  # 500 내부 서버 오류 처리\n",
    "def internal_error(error):\n",
    "    logger.error(f\"Internal Server Error: {str(error)}\")  # 오류 로그\n",
    "    return jsonify({\"error\": \"Internal Server Error\"}), 500  # 오류 메시지 반환\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(host='0.0.0.0', port=5000)  # 포트 5000에서 Flask 앱 시작\n",
    "    finally:\n",
    "        # 애플리케이션 종료 시 작업자 스레드 중지\n",
    "        torch.cuda.empty_cache()  # 메모리 캐시 비우기\n",
    "        for _ in range(num_worker_threads):\n",
    "            task_queue.put((None, None))  # 스레드에 중지 신호 전송\n",
    "        for t in threads:\n",
    "            t.join()  # 모든 스레드가 종료될 때까지 대기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d58b28d-5334-4835-b8bc-95707fba51a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Worker 0 starting, loading Whisper model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://localhost:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "100%|████████████████████████████████████████| 139M/139M [02:35<00:00, 934kiB/s]\n",
      "INFO:__main__:Worker 0 loaded Whisper model.\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 09:56:02] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpgwt0padc\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:__main__:Worker 0 processing task 1\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 09:56:16] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "ERROR:__main__:Worker 0 encountered an error: [WinError 2] 지정된 파일을 찾을 수 없습니다\n",
      "Exception in thread Thread-16 (worker):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_9324\\4189171189.py\", line 41, in worker\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py\", line 133, in transcribe\n",
      "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\audio.py\", line 140, in log_mel_spectrogram\n",
      "    audio = load_audio(audio)\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\audio.py\", line 58, in load_audio\n",
      "    out = run(cmd, capture_output=True, check=True).stdout\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 2] 지정된 파일을 찾을 수 없습니다\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_9324\\4189171189.py\", line 49, in worker\n",
      "NameError: name 'task_id' is not defined\n"
     ]
    }
   ],
   "source": [
    "import os  # 운영 체제와 상호작용하기 위한 os 모듈 임포트\n",
    "import tempfile  # 임시 파일 생성을 위한 tempfile 모듈 임포트\n",
    "import logging  # 로그 메시지를 위한 logging 모듈 임포트\n",
    "from flask import Flask, request, jsonify  # 웹 애플리케이션 생성을 위한 Flask와 유틸리티 임포트\n",
    "import whisper  # Whisper ASR 모델 임포트\n",
    "import torch  # 텐서 연산을 위한 PyTorch 임포트\n",
    "import threading  # 스레드를 생성하기 위한 threading 모듈 임포트\n",
    "import queue  # 작업 큐 생성을 위한 queue 모듈 임포트\n",
    "import uuid  # 고유한 작업 ID 생성을 위한 uuid 모듈 임포트\n",
    "import requests\n",
    "\n",
    "# 로그 메시지를 info 레벨로 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)  # 이 모듈을 위한 로거 생성\n",
    "\n",
    "app = Flask(__name__)  # Flask 애플리케이션 인스턴스 생성\n",
    "\n",
    "# 작업 큐와 결과 저장을 위한 딕셔너리 초기화\n",
    "task_queue = queue.Queue()\n",
    "results = {}\n",
    "\n",
    "# 오디오 파일을 처리할 작업자 스레드 수 설정\n",
    "num_worker_threads = 1\n",
    "\n",
    "# 오디오 파일을 처리할 작업자 함수 정의\n",
    "def worker(thread_id):\n",
    "    logger.info(f\"Worker {thread_id} starting, loading Whisper model...\")\n",
    "    model = whisper.load_model(\"base\")  # Whisper ASR 모델 로드\n",
    "    logger.info(f\"Worker {thread_id} loaded Whisper model.\")\n",
    "\n",
    "    while True:  # 작업자가 계속 실행되도록 무한 루프\n",
    "        note_id, audio_path = task_queue.get()  # 큐에서 작업 가져오기\n",
    "        if note_id is None:  # 종료 신호 확인\n",
    "            break  # 종료 신호를 받으면 루프 종료\n",
    "        \n",
    "        logger.info(f\"Worker {thread_id} processing task {note_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Whisper 모델을 사용하여 오디오 파일을 전사\n",
    "            # results[task_id] = {\"status\": \"processing\"}\n",
    "            result = model.transcribe(audio_path)\n",
    "            \n",
    "            send_result_to_spring(note_id, result['segments'])\n",
    "            \n",
    "            # results[task_id] = {\"status\": \"completed\", \"result\": result}  # 결과를 딕셔너리에 저장\n",
    "               \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Worker {thread_id} encountered an error: {str(e)}\")  # 발생한 오류 로그\n",
    "            results[note_id] = {\"status\": \"failed\", \"error\": str(e)}  # 오류를 결과에 저장\n",
    "        \n",
    "        finally:\n",
    "            os.remove(audio_path)  # 임시 오디오 파일 제거\n",
    "            if torch.cuda.is_available():  # CUDA 사용 가능 여부 확인\n",
    "                torch.cuda.empty_cache()  # 필요 시 CUDA 메모리 캐시 지우기\n",
    "        \n",
    "        task_queue.task_done()  # 작업 완료 표시\n",
    "\n",
    "def send_result_to_spring(note_id, result):\n",
    "    # 결과를 Spring 서버에 비동기로 전송\n",
    "    data = {\n",
    "        \"id\": note_id,  # note_id를 문자열로 변환하지 않음\n",
    "        \"result\": result  # result는 JSON 형식으로 예상\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\"http://localhost:8080/voice/stt\", json=data)\n",
    "        response.raise_for_status()  # 잘못된 응답에 대한 에러 발생\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"결과 전송 오류: {e}\")\n",
    "\n",
    "\n",
    "# 작업자 스레드 시작\n",
    "threads = []\n",
    "for i in range(num_worker_threads):\n",
    "    t = threading.Thread(target=worker, args=(i,))  # 작업자 함수에 대한 새 스레드 생성\n",
    "    t.start()  # 스레드 시작\n",
    "    threads.append(t)  # 스레드 목록에 추가\n",
    "\n",
    "@app.route('/', methods=['GET'])  # 홈 페이지를 위한 라우트 정의\n",
    "def home():\n",
    "    return \"Welcome to the Asynchronous Flask Server with Whisper!\"  # 환영 메시지 반환\n",
    "\n",
    "@app.route('/stt', methods=['POST'])  # STT 요청을 위한 라우트 정의\n",
    "def stt_request():\n",
    "    logger.info(\"Received STT request\")\n",
    "    \n",
    "    if not request.json.get('presigned_url'):  # 오디오 파일이 제공되었는지 확인\n",
    "        logger.warning(\"No audio file provided in the request\")\n",
    "        return jsonify({'error': 'No audio file provided'}), 400  # 제공되지 않으면 오류 반환\n",
    "\n",
    "    # https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
    "    audio_uri = request.json.get('presigned_url')  # 요청에서 오디오 파일 가져오기\n",
    "    note_id = request.json.get('id')\n",
    "    \n",
    "    if audio_uri:  # audio_uri가 제공된 경우\n",
    "        if not os.path.exists(audio_path):\n",
    "            print(f\"오디오 파일을 찾을 수 없습니다: {audio_path}\")\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Downloading audio file from URI: {audio_uri}\")\n",
    "            response = requests.get(audio_uri)  # URI에서 파일 다운로드\n",
    "            response.raise_for_status()  # 오류 발생 시 예외 처리\n",
    "            \n",
    "            # 오디오 데이터를 저장할 임시 파일 생성\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_audio:\n",
    "                temp_audio.write(response.content)  # 다운로드한 파일 내용을 임시 파일에 저장\n",
    "                temp_audio_path = temp_audio.name  # 임시 파일 경로 저장\n",
    "            \n",
    "            logger.info(f\"Audio file downloaded and saved to {temp_audio_path}\")\n",
    "\n",
    "            task_queue.put((note_id, temp_audio_path))  # 큐에 작업 추가\n",
    "\n",
    "            logger.info(f\"Task {note_id} added to the queue\")\n",
    "    \n",
    "            return jsonify({\"task_id\": note_id, \"status\": \"processing\"}), 202  # 작업 ID와 상태 반환\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Failed to download audio file: {str(e)}\")\n",
    "            return jsonify({'error': 'Failed to download audio file'}), 400          \n",
    "\n",
    "@app.errorhandler(500)  # 500 내부 서버 오류 처리\n",
    "def internal_error(error):\n",
    "    logger.error(f\"Internal Server Error: {str(error)}\")  # 오류 로그\n",
    "    return jsonify({\"error\": \"Internal Server Error\"}), 500  # 오류 메시지 반환\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(host='localhost', port=5000)  # 포트 5000에서 Flask 앱 시작\n",
    "    finally:\n",
    "        # 애플리케이션 종료 시 작업자 스레드 중지\n",
    "        torch.cuda.empty_cache()  # 메모리 캐시 비우기\n",
    "        for _ in range(num_worker_threads):\n",
    "            task_queue.put((None, None))  # 스레드에 중지 신호 전송\n",
    "        for t in threads:\n",
    "            t.join()  # 모든 스레드가 종료될 때까지 대기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f34fd14-07da-4cf7-b2f1-471f76487991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
