{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0a991-8d21-4459-b867-696eecbf1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712e2ab-76ff-4d4a-b67f-2eafadaef7c9",
   "metadata": {},
   "source": [
    "## 모델 상세 사용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd400de-89b7-45c4-b0f3-6141fff60428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"testAudio/noNoise_codeapple.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=128).to(model.device)  # n_mels를 128로 설정\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6b060-4d61-4af8-a532-a34b5013a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "\n",
    "# 모델 로드\n",
    "model = whisper.load_model(\"large\")\n",
    "\n",
    "# 오디오 파일 경로\n",
    "audio_file_path = \"testAudio/noNoise_codeapple.wav\"\n",
    "\n",
    "# 오디오 로드 및 샘플링\n",
    "audio = whisper.load_audio(audio_file_path)  # 오디오를 로드\n",
    "audio = whisper.pad_or_trim(audio)  # 필요 시 패딩 또는 트리밍\n",
    "\n",
    "# Mel 스펙트로그램 생성\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=128).to(model.device)\n",
    "\n",
    "# Mel 스펙트로그램 차원 확인\n",
    "print(f\"Mel shape: {mel.shape}\")\n",
    "\n",
    "# 언어 감지\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# 디코딩 옵션 설정\n",
    "options = whisper.DecodingOptions(language=\"ko\", task=\"transcribe\", without_timestamps=False)\n",
    "\n",
    "# 오디오 디코드 및 결과 저장\n",
    "result = model.decode(mel, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964cd785-ccf5-4955-8ce0-318d8783ccfc",
   "metadata": {},
   "source": [
    "## 모델 사용 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04d7f2-c621-4d2d-a18a-63992a24eb1d",
   "metadata": {},
   "source": [
    "### 모델 임포트 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f51595-efa4-4c05-bd32-91972a75980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "\n",
    "model = whisper.load_model(\"medium\") # large or medium 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3075190-140b-40b1-873e-2fd66eae73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = \"path/to/audio.wav\"\n",
    "\n",
    "audio = whisper.load_audio(origin)\n",
    "result = model.transcribe(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4632bd-01c1-4483-9e52-0b602cd01c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "large_model = whisper.load_model(\"large\")\n",
    "large_loading_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "base_model = whisper.load_model(\"base\")\n",
    "base_loading_time = time.time() - start_time\n",
    "\n",
    "print(f\"Large Model Loading Time: {large_loading_time:.2f} seconds\")\n",
    "print(f\"Base Model Loading Time: {base_loading_time:.2f} seconds\")\n",
    "\n",
    "# ============\n",
    "\n",
    "audio = whisper.load_audio(\"testAudio/noNoise_codeapple.wav\")\n",
    "\n",
    "start_time = time.time()\n",
    "large_model_output = large_model.transcribe(audio)\n",
    "large_inference_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "base_model_output = base_model.transcribe(audio)\n",
    "base_inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Large Model Inference Time: {large_inference_time:.2f} seconds\")\n",
    "print(f\"Base Model Inference Time: {base_inference_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166dbc2f-8e16-48f2-890a-b16b123c2509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 결과를 텍스트 파일로 저장\n",
    "with open(\"transcription_outputs.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    for model_name, output in outputs.items():\n",
    "        text_file.write(f\"{model_name} Output:\\n\")\n",
    "        text_file.write(f\"{output}\\n\\n\")  # 각 모델의 출력 사이에 공백 줄 추가\n",
    "\n",
    "print(\"Transcription outputs saved to transcription_outputs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cfbf7-05a9-465f-8e96-d639b15b3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b29dd7-637a-4bcc-9451-2e0c819eaa0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 시간 측정 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0805b3-39eb-4e03-ae74-417f0a1782f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "medium_model = whisper.load_model(\"medium\")\n",
    "medium_loading_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "small_model = whisper.load_model(\"small\")\n",
    "small_loading_time = time.time() - start_time\n",
    "\n",
    "print(f\"medium Model Loading Time: {medium_loading_time:.2f} seconds\")\n",
    "print(f\"small Model Loading Time: {small_loading_time:.2f} seconds\")\n",
    "\n",
    "# ============\n",
    "\n",
    "audio = whisper.load_audio(\"testAudio/noNoise_codeapple.wav\")\n",
    "\n",
    "start_time = time.time()\n",
    "medium_model_output = medium_model.transcribe(audio)\n",
    "medium_inference_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "small_model_output = small_model.transcribe(audio)\n",
    "small_inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"medium Model Inference Time: {medium_inference_time:.2f} seconds\")\n",
    "print(f\"small Model Inference Time: {small_inference_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73e845-16a5-4221-ba56-ce0d85a22799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio = whisper.load_audio(\"testAudio/noNpm.wav\")\n",
    "\n",
    "start_time = time.time()\n",
    "large_model_output = large_model.transcribe(audio)\n",
    "large_inference_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "base_model_output = base_model.transcribe(audio)\n",
    "base_inference_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "medium_model_output = medium_model.transcribe(audio)\n",
    "medium_inference_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "small_model_output = small_model.transcribe(audio)\n",
    "small_inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Large Model Inference Time: {large_inference_time:.2f} seconds\")\n",
    "print(f\"medium Model Inference Time: {medium_inference_time:.2f} seconds\")\n",
    "print(f\"small Model Inference Time: {small_inference_time:.2f} seconds\")\n",
    "print(f\"Base Model Inference Time: {base_inference_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75a176-a304-4297-9493-adc83107775c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af8440-61ec-4f7a-ad43-af34ad146fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm  # tqdm 라이브러리 추가\n",
    "import whisper\n",
    "\n",
    "# 오디오 파일 로드\n",
    "audio = whisper.load_audio(\"testAudio/Python_just_1Hour.mp3\")\n",
    "\n",
    "# 모델과 이름 리스트\n",
    "models = [\n",
    "    (\"Large Model\", large_model),\n",
    "    (\"Base Model\", base_model),\n",
    "    (\"Medium Model\", medium_model),\n",
    "    (\"Small Model\", small_model)\n",
    "]\n",
    "\n",
    "# 결과를 저장할 딕셔너리\n",
    "outputs = {}\n",
    "\n",
    "# 로딩 바를 사용하여 각 모델의 추론 시간을 측정\n",
    "for model_name, model in tqdm(models, desc=\"Transcribing\", unit=\"model\"):\n",
    "    start_time = time.time()\n",
    "    outputs[model_name] = model.transcribe(audio)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    tqdm.write(f\"{model_name} inference time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf001e8-ef57-469e-8e66-adc2fb7e124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_logprob(model_outputs):\n",
    "    avg_logprobs = []\n",
    "    \n",
    "    for model_output in model_outputs:\n",
    "        segments = model_output['segments']\n",
    "        avg_logprob = sum(segment['avg_logprob'] for segment in segments) / len(segments)\n",
    "        avg_logprobs.append(avg_logprob)\n",
    "    \n",
    "    return avg_logprobs\n",
    "\n",
    "model_outputs = [\n",
    "    base_model_output,\n",
    "    small_model_output,\n",
    "    medium_model_output,\n",
    "    large_model_output\n",
    "]\n",
    "\n",
    "avg_logprobs = calculate_avg_logprob(model_outputs)\n",
    "print(\"각 모델의 avg_logprob 비율:\", avg_logprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717b479-b626-40f4-b528-92ecbc5c8737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
